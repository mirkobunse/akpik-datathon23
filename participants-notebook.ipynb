{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AKPIK Datathon 2023\n",
    "\n",
    "Welcome to our Datathon! This Jupyter notebook loads the data for you and takes out a prior evaluation of your solutions. You can also study the data and build your solutions inside this notebook.\n",
    "\n",
    "**The data** consists of tiny grayscale images (28x28 pixels), which need to be classified into 10 classes. Below, we will take a look at some of these images.\n",
    "\n",
    "**Your goal** is to find those 10k training examples, from which the best classifier can be trained. The classifier is a fixed 3-layer neural network that is already implemented below. You are given a large pool of training data to select your 10k examples from. You are also given the features of the test set, but not the corresponding labels! Can you figure out the best training examples?\n",
    "\n",
    "## Basics: data loading and evaluation\n",
    "\n",
    "The following code implements our evaluation and downloads the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def evaluate(X_trn, y_trn, X_val, y_val, n_trials=10):\n",
    "    \"\"\"Evaluate the fixed model with given training and validation sets.\n",
    "    \n",
    "    The final evaluation pipeline of the Datathon is based on precisely the same function,\n",
    "    but employs the true testing data (X_tst, y_tst), of which the labels y_tst remain\n",
    "    undisclosed to you.\"\"\"\n",
    "    print(\"Evaluating the model...\")\n",
    "    performances = np.zeros(n_trials)\n",
    "    for i in range(n_trials): # evaluate the training procedure 10 times\n",
    "        model = tf.keras.Sequential([ # we use a simple, fully-connected, 3-layer neural network\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)), # inputs have 28x28 pixels\n",
    "            tf.keras.layers.Dense(256, activation=\"relu\"), # ReLU activation with 256 hidden units\n",
    "            tf.keras.layers.Dense(10) # the output are 10 class probabilities\n",
    "        ])\n",
    "        model.compile( # we will optimize for accuracy, using the standard ADAM optimizer\n",
    "            optimizer = \"adam\",\n",
    "            loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics = [\"accuracy\"]\n",
    "        )\n",
    "        model.fit(X_trn, y_trn, epochs=10, validation_split=.1, verbose=0) # train the neural network\n",
    "        performances[i] = model.evaluate(X_val, y_val, verbose=0)[1] # evaluate the neural network\n",
    "        print(f\"Testing accuracy [{i+1:02d}/{len(performances)}]: {performances[i]:.4f}\")\n",
    "    print(\"Average accuracy:\", np.mean(performances), \"+-\", np.std(performances))\n",
    "\n",
    "def download_data():\n",
    "    \"\"\"Download the training features, the training labels, and the testing features.\"\"\"\n",
    "    import os\n",
    "    import urllib.request\n",
    "    for data_path in [\"X_trn.npy\", \"y_trn.npy\", \"X_tst.npy\"]:\n",
    "        if not os.path.isfile(data_path):\n",
    "            url = \"https://tu-dortmund.sciebo.de/s/7Bespy29gx955hQ/download?files=\" + data_path\n",
    "            print(f\"Downloading {data_path} from {url}\")\n",
    "            urllib.request.urlretrieve(url, data_path)\n",
    "    print(\"All data is available now.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "def export_submission(train_indices, group_name):\n",
    "    \"\"\"\n",
    "    Export your submission data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_indices : np.ndarray[dtype=int]\n",
    "        The 10000 indices of your selected training events\n",
    "    group_name : str\n",
    "        the name of your group, should not contain spaces\n",
    "    \"\"\"\n",
    "    if train_indices.shape != (10000, ):\n",
    "        raise ValueError(\"You have to submit the indices of 10000 training events\")\n",
    "    \n",
    "    timestamp = datetime.now(timezone.utc).isoformat()\n",
    "    np.save(f\"submission_{group_name}_{timestamp}.npy\", train_indices)\n",
    "\n",
    "download_data() # execute the function that downloads the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple baseline solution\n",
    "\n",
    "How well would a random selection of 10k training examples work? The following code loads the data, selects the first 10k examples from the training pool, and evaluates this selection on the remaining data. It also stores this solution in a file - you should do the same with your solutions, to send them to the organizers of this Datathon for the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading the data...\")\n",
    "X_trn = np.load(\"X_trn.npy\") # this is a numpy matrix of the training features\n",
    "y_trn = np.load(\"y_trn.npy\") # this is a numpy array of the training labels\n",
    "X_tst = np.load(\"X_tst.npy\") # this is a numpy matrix of the testing features\n",
    "\n",
    "# baseline: select the first 10k examples\n",
    "i_trn = np.arange(10_000)\n",
    "\n",
    "# use all remaining data for validation\n",
    "#\n",
    "# hint: which data might be the most appropriate for validation?\n",
    "i_val = np.arange(10_000, len(y_trn))\n",
    "\n",
    "# evaluate the baseline\n",
    "evaluate(\n",
    "    X_trn[i_trn], y_trn[i_trn], # use the first 10k examples for training...\n",
    "    X_trn[i_val], y_trn[i_val] # ...and all remaining examples for validation.\n",
    ")\n",
    "\n",
    "# store your solution in a file that you can send to the organizers!\n",
    "np.save(\"solution_N_of_team_XYZ.npy\", i_trn) # always use this technique to store your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 60% of accuracy does not seem too bad for a 10-class classification task (random guessing instead of machine learning would only give us about 10% of accuracy here).\n",
    "\n",
    "However, you will find that this simple baseline will only achieve an accuracy of 42.5% on the actual testing set. Can you find out why the performance drops?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a look at the data\n",
    "\n",
    "You can use any tool you like to select your 10k training instances. Your first step in this process, however, should be to take a look at what the data represents. The following code plots the first 25 images of the training data with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_trn[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(y_trn[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like fashion! Let's also check what the class labels might represent. The following code plots the first 10 instances of class `9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_9 = X_trn[y_trn == 9] # select all instances of class 9\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in range(15):\n",
    "    plt.subplot(3,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_9[i], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these examples are boots!\n",
    "\n",
    "It is now your goal to find out, which training examples are best suited to train a machine learning model that performs well on the testing data `(X_tst, y_tst)`, without knowing `y_tst`.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proceed with any code you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base guess, just use the first 10000 events\n",
    "train_indices = np.arange(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export your training indices for submission\n",
    "export_submission(\n",
    "    train_indices=train_indices,\n",
    "    group_name=\"insert_group_name\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
